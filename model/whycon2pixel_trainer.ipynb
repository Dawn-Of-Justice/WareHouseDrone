{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most likely not at all used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also try to make a linear model to see if it is better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "import joblib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('output.csv')\n",
    "\n",
    "X = data[['X', 'Y', 'Z']]\n",
    "y = data[['PixelX', 'PixelY']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPRegressor(hidden_layer_sizes=(10,10), max_iter=1, warm_start=True)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2500):  \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    train_pred = model.predict(X_train_scaled)\n",
    "    test_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    train_loss = mean_squared_error(y_train, train_pred)\n",
    "    test_loss = mean_squared_error(y_test, test_pred)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    if i%100 == 1:\n",
    "        print('Loss:', train_loss, test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.title('Train and Test Loss Curve')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, 'NN_model_xyz_to_XY.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from warnings import filterwarnings\n",
    "scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load('NN_model_xyz_to_XY.pkl')\n",
    "sample_input = [[0.5, -1.2, 3.4]]\n",
    "sample_input_scaled = scaler.transform(sample_input)\n",
    "prediction = loaded_model.predict(sample_input_scaled)\n",
    "\n",
    "print(\"Predicted PixelX, PixelY:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('NN_model_xyz_to_XY.pkl')\n",
    "\n",
    "# Define the input data (X, Y, Z values)\n",
    "input_data = np.array([\n",
    "    [-8.179, 1.226, 26.594],\n",
    "    [-8.139, 2.576, 26.746],\n",
    "    [-8.289, 4.123, 27.328],\n",
    "    [-8.283, 5.653, 27.431],\n",
    "    [-8.238, 7.08, 27.312],\n",
    "    [-8.256, 8.501, 27.454],\n",
    "    [-7.866, 9.739, 26.589],\n",
    "    [-8.279, -10.466, 26.065]\n",
    "])\n",
    "\n",
    "# Original PixelX, PixelY values for comparison\n",
    "original_outputs = np.array([\n",
    "    [54, 560],\n",
    "    [52, 624],\n",
    "    [52, 682],\n",
    "    [49, 757],\n",
    "    [54, 810],\n",
    "    [61, 879],\n",
    "    [59, 938],\n",
    "    [57, 51]\n",
    "])\n",
    "\n",
    "# Scale the input data\n",
    "input_data_scaled = scaler.transform(input_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = loaded_model.predict(input_data_scaled)\n",
    "\n",
    "# Print results\n",
    "print(\"Predicted vs Original PixelX, PixelY values:\")\n",
    "for i, (pred, orig) in enumerate(zip(predictions, original_outputs)):\n",
    "    print(f\"Point {i+1}: Predicted {pred}, Original {orig}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from warnings import filterwarnings\n",
    "import joblib\n",
    "\n",
    "filterwarnings('ignore')\n",
    "\n",
    "class CoordinatePredictor:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.nn_model = None\n",
    "        self.linear_model = None\n",
    "        \n",
    "    def prepare_data(self, data_path):\n",
    "        \"\"\"Load and prepare data for training\"\"\"\n",
    "        data = pd.read_csv(data_path)\n",
    "        X = data[['X', 'Y', 'Z']]\n",
    "        y = data[['PixelX', 'PixelY']]\n",
    "        \n",
    "        return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    def train_neural_network(self, X_train, X_test, y_train, y_test, \n",
    "                           hidden_layers=(10,10), max_iterations=10000):\n",
    "        \"\"\"Train neural network model and return losses\"\"\"\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        self.nn_model = MLPRegressor(\n",
    "            hidden_layer_sizes=hidden_layers,\n",
    "            max_iter=1,\n",
    "            warm_start=True,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        \n",
    "        for i in range(max_iterations):\n",
    "            self.nn_model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            train_pred = self.nn_model.predict(X_train_scaled)\n",
    "            test_pred = self.nn_model.predict(X_test_scaled)\n",
    "            \n",
    "            train_loss = mean_squared_error(y_train, train_pred)\n",
    "            test_loss = mean_squared_error(y_test, test_pred)\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f'Iteration {i}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "        \n",
    "        return train_losses, test_losses\n",
    "    \n",
    "    def train_linear_model(self, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"Train linear regression model and return metrics\"\"\"\n",
    "        X_train_scaled = self.scaler.transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        self.linear_model = LinearRegression()\n",
    "        self.linear_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        train_pred = self.linear_model.predict(X_train_scaled)\n",
    "        test_pred = self.linear_model.predict(X_test_scaled)\n",
    "        \n",
    "        train_loss = mean_squared_error(y_train, train_pred)\n",
    "        test_loss = mean_squared_error(y_test, test_pred)\n",
    "        r2 = r2_score(y_test, test_pred)\n",
    "        \n",
    "        return train_loss, test_loss, r2\n",
    "    \n",
    "    def plot_nn_losses(self, train_losses, test_losses):\n",
    "        \"\"\"Plot neural network training curves\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(test_losses, label='Test Loss')\n",
    "        plt.title('Neural Network Training and Test Loss Curves')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Mean Squared Error')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    def save_models(self, nn_path='NN_model.pkl', linear_path='Linear_model.pkl'):\n",
    "        \"\"\"Save both models to disk\"\"\"\n",
    "        if self.nn_model:\n",
    "            joblib.dump(self.nn_model, nn_path)\n",
    "        if self.linear_model:\n",
    "            joblib.dump(self.linear_model, linear_path)\n",
    "    \n",
    "    def load_models(self, nn_path='NN_model.pkl', linear_path='Linear_model.pkl'):\n",
    "        \"\"\"Load models from disk\"\"\"\n",
    "        self.nn_model = joblib.load(nn_path)\n",
    "        self.linear_model = joblib.load(linear_path)\n",
    "    \n",
    "    def predict(self, input_data, model_type='linear'):\n",
    "        \"\"\"Make predictions using specified model\"\"\"\n",
    "        input_scaled = self.scaler.transform(input_data)\n",
    "        \n",
    "        if model_type.lower() == 'nn' and self.nn_model:\n",
    "            return self.nn_model.predict(input_scaled)\n",
    "        elif model_type.lower() == 'linear' and self.linear_model:\n",
    "            return self.linear_model.predict(input_scaled)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model type or model not trained\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Neural Network...\n",
      "Iteration 0: Train Loss: 313988.1599, Test Loss: 359381.1168\n",
      "Iteration 100: Train Loss: 313226.1526, Test Loss: 358418.4198\n",
      "Iteration 200: Train Loss: 311644.5138, Test Loss: 356464.2582\n",
      "Iteration 300: Train Loss: 308633.4562, Test Loss: 352827.9213\n",
      "Iteration 400: Train Loss: 303493.4731, Test Loss: 346612.4255\n",
      "Iteration 500: Train Loss: 295583.0949, Test Loss: 337032.3448\n",
      "Iteration 600: Train Loss: 284410.1235, Test Loss: 323510.9186\n",
      "Iteration 700: Train Loss: 269600.3924, Test Loss: 305559.7760\n",
      "Iteration 800: Train Loss: 250930.6226, Test Loss: 282894.3939\n",
      "Iteration 900: Train Loss: 228503.0758, Test Loss: 255668.9136\n",
      "Iteration 1000: Train Loss: 202698.5736, Test Loss: 224347.6972\n",
      "Iteration 1100: Train Loss: 174324.4740, Test Loss: 189807.4578\n",
      "Iteration 1200: Train Loss: 144728.1408, Test Loss: 153758.9364\n",
      "Iteration 1300: Train Loss: 115595.8300, Test Loss: 118709.5731\n",
      "Iteration 1400: Train Loss: 88573.7552, Test Loss: 86754.5262\n",
      "Iteration 1500: Train Loss: 66092.7093, Test Loss: 60226.8672\n",
      "Iteration 1600: Train Loss: 51420.9690, Test Loss: 43970.1102\n",
      "Iteration 1700: Train Loss: 44328.5302, Test Loss: 37654.2231\n",
      "Iteration 1800: Train Loss: 39720.1262, Test Loss: 33588.0792\n",
      "Iteration 1900: Train Loss: 35372.9370, Test Loss: 29388.0865\n",
      "Iteration 2000: Train Loss: 30846.6709, Test Loss: 25190.6728\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Train and evaluate neural network\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Neural Network...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m train_losses, test_losses \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_neural_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m predictor\u001b[38;5;241m.\u001b[39mplot_nn_losses(train_losses, test_losses)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train and evaluate linear model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 50\u001b[0m, in \u001b[0;36mCoordinatePredictor.train_neural_network\u001b[0;34m(self, X_train, X_test, y_train, y_test, hidden_layers, max_iterations)\u001b[0m\n\u001b[1;32m     47\u001b[0m train_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_model\u001b[38;5;241m.\u001b[39mpredict(X_train_scaled)\n\u001b[1;32m     48\u001b[0m test_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_model\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled)\n\u001b[0;32m---> 50\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m mean_squared_error(y_test, test_pred)\n\u001b[1;32m     53\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m validate_parameter_constraints(\n\u001b[1;32m    207\u001b[0m     parameter_constraints, params, caller_name\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\n\u001b[1;32m    208\u001b[0m )\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:130\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_GeneratorContextManager\u001b[39;00m(\n\u001b[1;32m    124\u001b[0m     _GeneratorContextManagerBase,\n\u001b[1;32m    125\u001b[0m     AbstractContextManager,\n\u001b[1;32m    126\u001b[0m     ContextDecorator,\n\u001b[1;32m    127\u001b[0m ):\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;124;03m\"\"\"Helper for @contextmanager decorator.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;66;03m# do not keep args and kwds alive unnecessarily\u001b[39;00m\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;66;03m# they are only needed for recreation, which is not possible anymore\u001b[39;00m\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictor = CoordinatePredictor()\n",
    "\n",
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test = predictor.prepare_data('output.csv')\n",
    "\n",
    "# Train and evaluate neural network\n",
    "print(\"\\nTraining Neural Network...\")\n",
    "train_losses, test_losses = predictor.train_neural_network(X_train, X_test, y_train, y_test)\n",
    "predictor.plot_nn_losses(train_losses, test_losses)\n",
    "\n",
    "# Train and evaluate linear model\n",
    "print(\"\\nTraining Linear Model...\")\n",
    "train_loss, test_loss, r2 = predictor.train_linear_model(X_train, X_test, y_train, y_test)\n",
    "print(f\"Linear Model Metrics:\")\n",
    "print(f\"Train MSE: {train_loss:.4f}\")\n",
    "print(f\"Test MSE: {test_loss:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Save models\n",
    "predictor.save_models()\n",
    "\n",
    "# Example prediction\n",
    "sample_input = np.array([[-8.179, 1.226, 26.594]])\n",
    "nn_pred = predictor.predict(sample_input, 'nn')\n",
    "linear_pred = predictor.predict(sample_input, 'linear')\n",
    "\n",
    "print(\"\\nPredictions for sample input:\")\n",
    "print(f\"Neural Network: {nn_pred}\")\n",
    "print(f\"Linear Model: {linear_pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted PixelX, PixelY: [[661.84967933 488.31360644]]\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the actual training range (min/max values from the dataset)\n",
    "training_range = np.array([\n",
    "    [-8.404, -10.725, 25.021],  # min values (X, Y, Z)\n",
    "    [11.644, 10.137, 27.974]    # max values (X, Y, Z)\n",
    "])\n",
    "# Create and fit scaler with training range\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(training_range)\n",
    "\n",
    "# Load model\n",
    "model = joblib.load('NN_model_xyz_to_XY.pkl')\n",
    "\n",
    "# Example input (replace with your coordinates)\n",
    "input_data = np.array([\n",
    "    [9.975,-0.33,26.099,]\n",
    "])\n",
    "\n",
    "# Scale and predict\n",
    "input_scaled = scaler.transform(input_data)\n",
    "predictions = model.predict(input_scaled)\n",
    "print(\"Predicted PixelX, PixelY:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The real one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PixelX R² score: 0.9996\n",
      "PixelY R² score: 0.9996\n",
      "\n",
      "PixelX coefficients: [273.89924623  -3.47144254  -0.80198502]\n",
      "PixelX intercept: 497.85333333333335\n",
      "\n",
      "PixelY coefficients: [  4.09447346 275.09379539  -0.7497379 ]\n",
      "PixelY intercept: 497.4488888888889\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Read data\n",
    "data = pd.read_csv('output.csv')\n",
    "\n",
    "# Prepare features and targets\n",
    "X = data[['X', 'Y', 'Z']]\n",
    "y_pixelX = data['PixelX']\n",
    "y_pixelY = data['PixelY']\n",
    "\n",
    "# Create and fit scalers\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train models for X and Y pixel coordinates\n",
    "model_pixelX = LinearRegression()\n",
    "model_pixelY = LinearRegression()\n",
    "\n",
    "model_pixelX.fit(X_scaled, y_pixelX,)\n",
    "model_pixelY.fit(X_scaled, y_pixelY)\n",
    "\n",
    "# Print R² scores\n",
    "print(f\"PixelX R² score: {model_pixelX.score(X_scaled, y_pixelX):.4f}\")\n",
    "print(f\"PixelY R² score: {model_pixelY.score(X_scaled, y_pixelY):.4f}\")\n",
    "\n",
    "# Save models and scaler\n",
    "joblib.dump(model_pixelX, 'model_pixelX.pkl')\n",
    "joblib.dump(model_pixelY, 'model_pixelY.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# Optional: Print coefficients\n",
    "print(\"\\nPixelX coefficients:\", model_pixelX.coef_)\n",
    "print(\"PixelX intercept:\", model_pixelX.intercept_)\n",
    "print(\"\\nPixelY coefficients:\", model_pixelY.coef_)\n",
    "print(\"PixelY intercept:\", model_pixelY.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 1: [-6.29 -5.35 26.  ]\n",
      "Predicted Pixel X: 146.7, Pixel Y: 271.5\n",
      "\n",
      "Input 2: [ 4.07 -5.75 26.  ]\n",
      "Predicted Pixel X: 613.8, Pixel Y: 260.8\n",
      "\n",
      "Input 3: [-8.07  5.31 26.  ]\n",
      "Predicted Pixel X: 60.5, Pixel Y: 740.9\n",
      "\n",
      "Input 4: [ 8.04  1.83 26.  ]\n",
      "Predicted Pixel X: 788.5, Pixel Y: 598.1\n",
      "\n",
      "Input 5: [ 1.26  4.12 26.  ]\n",
      "Predicted Pixel X: 481.7, Pixel Y: 694.7\n",
      "\n",
      "Input 6: [ 7.45  9.88 26.  ]\n",
      "Predicted Pixel X: 757.5, Pixel Y: 953.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Load models and scaler\n",
    "model_pixelX = joblib.load('model_pixelX.pkl')\n",
    "model_pixelY = joblib.load('model_pixelY.pkl')\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "input_data = np.array([\n",
    "    [-6.29, -5.35, 26.0],  # Point 1\n",
    "    [4.07, -5.75, 26.0],   # Point 2\n",
    "    [-8.07, 5.31, 26.0],   # Point 3\n",
    "    [8.04, 1.83, 26.0],    # Point 4\n",
    "    [1.26, 4.12, 26.0],    # Point 5\n",
    "    [7.45, 9.88, 26.0]     # Point 6\n",
    "])\n",
    "\n",
    "# Scale input\n",
    "input_scaled = scaler.transform(input_data)\n",
    "\n",
    "# Predict\n",
    "pixel_x = model_pixelX.predict(input_scaled)\n",
    "pixel_y = model_pixelY.predict(input_scaled)\n",
    "\n",
    "# Print results\n",
    "for i in range(len(input_data)):\n",
    "    print(f\"Input {i+1}: {input_data[i]}\")\n",
    "    print(f\"Predicted Pixel X: {pixel_x[i]:.1f}, Pixel Y: {pixel_y[i]:.1f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
